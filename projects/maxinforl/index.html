<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>MaxInfoRL - Boosting exploration in reinforcement learning through information gain maximization | Bhavya Sukhija</title>
    <meta name="author" content="Bhavya  Sukhija">
    <meta name="description" content="Personal webpage of Bhavya Sukhija.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sukhijab.github.io/projects/maxinforl/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "MaxInfoRL - Boosting exploration in reinforcement learning through information gain maximization",
      "description": "",
      "published": "December 16, 2024",
      "authors": [
        {
          "author": "Bhavya Sukhija",
          "authorURL": "https://sukhijab.github.io",
          "affiliations": [
            {
              "name": "LAS<d-footnote>Learning & Adaptive Systems Group</d-footnote> & CRL<d-footnote>Computational Robotics Lab</d-footnote>, ETH Zurich",
              "url": ""
            }
          ]
        },
        {
          "author": "Stelian Coros",
          "authorURL": "http://crl.ethz.ch/people/coros/index.html",
          "affiliations": [
            {
              "name": "CRL, ETH Zurich",
              "url": ""
            }
          ]
        },
        {
          "author": "Andreas Krause",
          "authorURL": "https://las.inf.ethz.ch/krausea",
          "affiliations": [
            {
              "name": "LAS, ETH Zurich",
              "url": ""
            }
          ]
        },
        {
          "author": "Pieter Abbeel",
          "authorURL": "https://people.eecs.berkeley.edu/~pabbeel/",
          "affiliations": [
            {
              "name": "RLL<d-footnote>Robot Learning Lab</d-footnote>, UC Berkeley",
              "url": ""
            }
          ]
        },
        {
          "author": "Carlo Sferrazza",
          "authorURL": "https://sferrazza.cc",
          "affiliations": [
            {
              "name": "RLL, UC Berkeley",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Bhavya Sukhija</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About Me</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>MaxInfoRL - Boosting exploration in reinforcement learning through information gain maximization</h1>
        <p></p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <p>Paper: <a href="https://openreview.net/pdf?id=R4q3cY3kQf" rel="external nofollow noopener" target="_blank"> Arxiv.</a></p>

<p>Implementation: <a href="https://github.com/sukhijab/maxinforl_jax" rel="external nofollow noopener" target="_blank">Jax</a>, <a href="https://github.com/sukhijab/maxinforl_torch" rel="external nofollow noopener" target="_blank">PyTorch</a>.</p>

<p><img src="/assets/slides/maxinforl/teaser.gif" width="720"></p>

<h2 id="abstract">Abstract</h2>

<p>Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.</p>

<h2 id="problem-setting">Problem Setting</h2>

<p>We study the problem of online reinforcement learning (RL), where the agent interacts with the environment and uses the data collected from these interactions to improve itself.</p>

<p><img src="/assets/slides/maxinforl/intro.gif" width="720"></p>

<p>A core challenge in this setting is deciding whether the agent should leverage its current knowledge to maximize performance or try new actions in pursuit of better solutions. Striking this balance between exploration–exploitation is critical.</p>

<p>Most widely applied online RL algorithms, such as SAC, explore naively. They fail to account for the agent’s lack of knowledge; instead, the agent explores in an undirected fashion by sampling random action sequences. This leads to suboptimal performance, particularly in challenging exploration tasks with continuous state-action spaces. Nonetheless, such algorithms are ubiquitous in RL research due to their simplicity. In this work, we boost these algorithms with directed exploration derived from information gain. Thereby, we maintain the simplicity of the naive exploration methods but obtain better and directed exploration.</p>

<h2 id="maxinforl">MaxInfoRL</h2>

<!---
The basis of many RL algorithms is Boltzmann exploration, where the policy $\pi: \mathcal{A} \times \mathcal{S} \to \mathbb{R}_{+}$ has the following distribution:

$$
\pi(a \mid s) \propto \exp\left(\alpha^{-1} Q^{\pi}(s, a)\right),
$$

here $Q^{\pi}(s, a)$ is the soft-Q function that measures the performance of the policy with respect to the extrinsic rewards.

For small values of the temperature $\alpha$, the policy acts greedily with respect to the Q-function, and for large values of $\alpha$, it acts uniformly at random. To this end, the temperature is used to trade off exploration–exploitation.

The above distribution is the basis of several RL algorithms (SAC, REDQ, DrQ), where the policy is picked to maximize the following objective:

$$
\max_{\pi} \mathbb{E}_{a \sim \pi}\left[ Q^{\pi}(s, a) - \alpha \log(\pi(a \mid s)) \right].
$$

In MaxInfoRL, we augment the distribution above and propose:

$$
\pi(a \mid s) \propto \exp\left(\alpha^{-1} Q^{\pi}(s, a) + I(s^{\prime}; f^{*} \mid s, a)\right),
$$


here, $I(s^{\prime}; f^{*} \mid s, a)$ represents the information gain from observing the tuple $(s, a, s^{\prime})$. For small values of the temperature $\alpha$, the policy still remains greedy with respect to the Q-function, but for large values of $\alpha$, it focuses on collecting informative samples, i.e., explores in a directed fashion. The policy is picked to maximize the following objective:

$$
\max_{\pi} \mathbb{E}_{a \sim \pi}\left[ Q^{\pi}(s, a) - \alpha \log(\pi(a \mid s)) + \alpha I(s^{\prime}; f^{*} \mid s, a)\right].
$$
-->

<p>We combine Boltzmann exploration based soft-actor critic algorithms (<a href="https://arxiv.org/pdf/1812.05905" rel="external nofollow noopener" target="_blank">SAC</a>, <a href="https://arxiv.org/pdf/2101.05982" rel="external nofollow noopener" target="_blank">REDQ</a>, <a href="https://arxiv.org/pdf/2004.13649" rel="external nofollow noopener" target="_blank">DrQ</a> etc) with intrinsic rewards to obtain principled exploration. In particular, we use an ensemble of forward dynamics model to estimate the information gain of each transition tuple and incorporate it as an exploration bonus in addition to the policy entropy. 
This results in the policy update rule described below.
<img src="/assets/slides/maxinforl/info_gain_animation.gif" width="720"></p>

<p>The temperatures $(\alpha_1, \alpha_2)$ are automatically tuned as typically done for soft actor-critic algorithms.</p>

<h3 id="maxinforl-maximizes-state-entropy">MaxInfoRL maximizes state entropy</h3>
<p>While the standard Boltzmann exploration focuses only on the entropy of actions, due to the introduction of the information gain, MaxInfoRL also maximizes the state entropy. This is depcited in the simple graphic below. Where we illustrate on the example of a pendulum that MaxInfoRL covers the state space much more.
<img src="/assets/slides/maxinforl/pendulum_animation.gif" width="720"></p>

<h2 id="experiments">Experiments</h2>

<h3 id="maxinforl-is-flexible">MaxInfoRL is flexible!</h3>

<p>MaxInfoRL can be combined with different RL algorithms and intrinsic rewards. To illustrate this, we combine it with <a href="https://arxiv.org/pdf/1812.05905" rel="external nofollow noopener" target="_blank">SAC</a>, <a href="https://arxiv.org/pdf/2101.05982" rel="external nofollow noopener" target="_blank">REDQ</a>, <a href="https://arxiv.org/pdf/2004.13649" rel="external nofollow noopener" target="_blank">DrQ</a>, and <a href="https://arxiv.org/pdf/2107.09645" rel="external nofollow noopener" target="_blank">DrQv2</a>. Furthermore, we also evaluate MaxInfoRL with RND as intrinsic reward. In all cases, the algorithm automatically trades-off extrinsic rewards with intrinsic exploration and outperforms the base algorithm.
<img src="/assets/slides/maxinforl/flexibility.png" width="720"></p>

<h3 id="maxinforl-is-sota-on-hard-visual-control-tasks">MaxInfoRL is SOTA on hard visual control tasks!</h3>

<p>We evaluate MaxInfoRL on hard visual control tasks from the deepmind control suite and show that it outperforms the SOTA: <a href="https://arxiv.org/pdf/2310.19668" rel="external nofollow noopener" target="_blank">DrM</a>. 
<img src="/assets/slides/maxinforl/visual_control.gif" width="720"></p>

<h2 id="bibtex">Bibtex</h2>

<div class="code-container" style="position: relative;">
    <button id="copy-btn" onclick="copyToClipboard()" style="position: absolute; top: 10px; right: 10px; background-color: #f0f0f0; border: none; padding: 5px 10px; cursor: pointer; border-radius: 5px;">
        <i id="copy-icon" class="fa fa-copy" style="font-size: 16px;"></i>
    </button>
    <pre id="bibtex-entry"><code>
@article{sukhija2024maxinforl,
  title={MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization},
  author={Sukhija, Bhavya and Coros, Stelian and Krause, Andreas and Abbeel, Pieter and Sferrazza, Carmelo},
  journal={ArXiv},
  year={2024}
}
    </code></pre>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"></script>

<script>
function copyToClipboard() {
    var codeText = document.getElementById('bibtex-entry').innerText;
    var textArea = document.createElement('textarea');
    textArea.value = codeText;
    document.body.appendChild(textArea);
    textArea.select();
    document.execCommand('copy');
    document.body.removeChild(textArea);

    // Change the icon to the checkmark
    var copyIcon = document.getElementById('copy-icon');
    copyIcon.classList.remove('fa-copy');
    copyIcon.classList.add('fa-check');  // Switch to checkmark icon

    // Change it back to the copy icon after 2 seconds
    setTimeout(function() {
        copyIcon.classList.remove('fa-check');
        copyIcon.classList.add('fa-copy');  // Switch back to copy icon
    }, 350);
}
</script>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Bhavya  Sukhija. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
