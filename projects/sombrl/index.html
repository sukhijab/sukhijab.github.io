<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Scalable and Optimistic Model-Based RL maximization | Bhavya Sukhija</title>
    <meta name="author" content="Bhavya  Sukhija">
    <meta name="description" content="Personal webpage of Bhavya Sukhija.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sukhijab.github.io/projects/sombrl/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Scalable and Optimistic Model-Based RL maximization",
      "description": "",
      "published": "May 8, 2025",
      "authors": [
        {
          "author": "Bhavya Sukhija",
          "authorURL": "https://sukhijab.github.io",
          "affiliations": [
            {
              "name": "LAS<d-footnote>Learning & Adaptive Systems Group</d-footnote> & CRL<d-footnote>Computational Robotics Lab</d-footnote>, ETH Zurich",
              "url": ""
            }
          ]
        },
        {
          "author": "Lenart Treven",
          "authorURL": "https://lenarttreven.github.io/",
          "affiliations": [
            {
              "name": "LAS, ETH Zurich",
              "url": ""
            }
          ]
        },
        {
          "author": "Carlo Sferrazza",
          "authorURL": "https://sferrazza.cc",
          "affiliations": [
            {
              "name": "RLL<d-footnote>Robot Learning Lab</d-footnote>, UC Berkeley",
              "url": ""
            }
          ]
        },
        {
          "author": "Florian Dörfler",
          "authorURL": "https://dorfler.ethz.ch/",
          "affiliations": [
            {
              "name": "ACL (IfA)<d-footnote>Automatic Control Laboratory</d-footnote> , ETH Zurich",
              "url": ""
            }
          ]
        },
        {
          "author": "Pieter Abbeel",
          "authorURL": "https://people.eecs.berkeley.edu/~pabbeel/",
          "affiliations": [
            {
              "name": "RLL, UC Berkeley",
              "url": ""
            }
          ]
        },
        {
          "author": "Andreas Krause",
          "authorURL": "https://las.inf.ethz.ch/krausea",
          "affiliations": [
            {
              "name": "LAS, ETH Zurich",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Bhavya Sukhija</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About Me</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Scalable and Optimistic Model-Based RL maximization</h1>
        <p></p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <p>Paper: <a href="https://openreview.net/pdf?id=eGfi5k7RP6" rel="external nofollow noopener" target="_blank"> NeurIPS2025.<br>
Implementation: </a><a href="https://github.com/lasgroup/ombrl" rel="external nofollow noopener" target="_blank">MBPO</a></p>

<h2 id="abstract">Abstract</h2>

<p>We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent’s epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic setting. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.</p>

<h2 id="problem-setting">Problem Setting</h2>

<p>We study the problem of online reinforcement learning (RL), where the agent interacts with the environment and uses the data collected from these interactions to improve itself.</p>

<div style="text-align: center;">
<img src="/assets/slides/sombrl/online_rl_loop.gif" width="720">
</div>

<p>Exploration–Exploitation trade-off plays a crucial role in this setting: 
Should the agent leverage its current knowledge to maximize performance or try new actions in pursuit of better solutions?</p>

<p>Most widely applied online RL algorithms explore naively and accordingly yield suboptimal performance. On the other hand, principled RL methods, despite their strong theoretical foundations, do not scale to real-world settings and therefore are rarely applied. 
In this work, we bridge the gap between theory and practice and propose a principled yet scalable algorithm for online RL.</p>

<h2 id="sombrl">SOMBRL</h2>

<!---
The basis of many RL algorithms is Boltzmann exploration, where the policy $\pi: \mathcal{A} \times \mathcal{S} \to \mathbb{R}_{+}$ has the following distribution:

$$
\pi(a \mid s) \propto \exp\left(\alpha^{-1} Q^{\pi}(s, a)\right),
$$

here $Q^{\pi}(s, a)$ is the soft-Q function that measures the performance of the policy with respect to the extrinsic rewards.

For small values of the temperature $\alpha$, the policy acts greedily with respect to the Q-function, and for large values of $\alpha$, it acts uniformly at random. To this end, the temperature is used to trade off exploration–exploitation.

The above distribution is the basis of several RL algorithms (SAC, REDQ, DrQ), where the policy is picked to maximize the following objective:

$$
\max_{\pi} \mathbb{E}_{a \sim \pi}\left[ Q^{\pi}(s, a) - \alpha \log(\pi(a \mid s)) \right].
$$

In MaxInfoRL, we augment the distribution above and propose:

$$
\pi(a \mid s) \propto \exp\left(\alpha^{-1} Q^{\pi}(s, a) + I(s^{\prime}; f^{*} \mid s, a)\right),
$$


here, $I(s^{\prime}; f^{*} \mid s, a)$ represents the information gain from observing the tuple $(s, a, s^{\prime})$. For small values of the temperature $\alpha$, the policy still remains greedy with respect to the Q-function, but for large values of $\alpha$, it focuses on collecting informative samples, i.e., explores in a directed fashion. The policy is picked to maximize the following objective:

$$
\max_{\pi} \mathbb{E}_{a \sim \pi}\left[ Q^{\pi}(s, a) - \alpha \log(\pi(a \mid s)) + \alpha I(s^{\prime}; f^{*} \mid s, a)\right].
$$
-->

<p>We combine greedy exploration methods with intrinsic rewards obtained from the epistemic uncertainty of a learned dynamics model. We use an ensemble of forward dynamics model to estimate the model uncertainty of each transition tuple and incorporate it as an exploration bonus. The learned model is then also used for planning yielding additional gains in sample-efficiency. 
This results in the policy update rule described below.</p>

\[\pi_n = \underset{\pi \in \Pi}{\arg\max}\; \mathbb{E}_{\pi} \left[ \sum^{T-1}_{t=0} r(x'_t, u_t) + \lambda_n ||\sigma_n(x'_t, u_t)|| \right],
\; x'_{t+1} = \mu_n(x'_t, u_t) + w_t,\]

<h3 id="sombrl-has-sublinear-regret-for-most-common-rl-settings">SOMBRL has sublinear regret for most common RL settings</h3>
<p>We theoretically investigate SOMBRL and show that it represents a scalable approach for optimistic exploration in model-based RL. Moreover, we show under regularlity assumptions of the underlying system, that SOMBRL enjoys sublinear regret for the most common RL settings.</p>

<p><strong>Main Theorem</strong>: Under regularlity assumptions on the system, SOMBRL has sublinear (polylogarithmic) regret $R_N = \sum^N_{n=1} r_n$ for (i) finite-horizon, (ii) discounted infinite horizon, and (iii) non-episodic average reward settings. Here $r_n$ measures the performance gap between the optimal and current policy.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="sombrl-is-more-scalable-than-other-principled-exploration-methods">SOMBRL is more scalable than other principled exploration methods!</h3>

<p>We compare SOMBRL with other principled exploration algorithms in the episodic <a href="https://arxiv.org/abs/2006.08684" rel="external nofollow noopener" target="_blank">HUCRL</a> and nonepisodic setting <a href="https://arxiv.org/abs/2406.01175" rel="external nofollow noopener" target="_blank">NeORL</a>. Due to the simplified optimization of SOMBRL, it outperfroms these principled methods and is also computationally cheaper!</p>
<div style="text-align: center;">
<img src="/assets/slides/sombrl/sombrl_optimism_comparison.png" width="720">
</div>

<h3 id="sombrl-performs-well-across-the-board">SOMBRL performs well across the board!</h3>

<p>We compare SOMBRL with other deep RL model-based methods on state-based (<a href="https://arxiv.org/abs/1906.08253" rel="external nofollow noopener" target="_blank">MBPO</a>) and visual control tasks (<a href="https://arxiv.org/abs/2301.04104" rel="external nofollow noopener" target="_blank">Dreamerv3</a>). 
Principled exploration methods such as <a href="https://arxiv.org/abs/2006.08684" rel="external nofollow noopener" target="_blank">HUCRL</a>, do not scale to these settings. 
Whereas scalable methods such as <a href="https://arxiv.org/abs/1906.08253" rel="external nofollow noopener" target="_blank">MBPO</a> and <a href="https://arxiv.org/abs/2301.04104" rel="external nofollow noopener" target="_blank">Dreamerv3</a> explore naively, i.e., are not principled.<br>
However, due to SOMBRL’s practical approach to optimistic exploration, it can be seamlessly applied to high-dimensional settings. In addition, SOMBRL is also principled and enjoys theoretical guarantees across several common RL settings. Finally, across all our experiments, SOMBRL performs the best or on-par than the SOTA deep RL baselines, while also having negligible difference in computational cost.</p>
<div style="text-align: center;">
<img src="/assets/slides/sombrl/main_results_sombrl.png" width="720">
</div>

<p></p>
<p><strong>Computational Cost</strong></p>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th>Training time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>HUCRL (GPs)</td>
      <td>90 +/- 3 min (Pendulum), 31.5 +/- 2.5 min (MountainCar)</td>
    </tr>
    <tr>
      <td>SOMBRL (GPs)</td>
      <td>30 +/- 0.6 min (Pendulum), 13.8 +/- 0.25 min (MountainCar)</td>
    </tr>
    <tr>
      <td>MBPO-Mean</td>
      <td>9.6 +/- 0.2 min (Time per 100k steps, 1 ensemble, GPU: NVIDIA GeForce RTX 2080 Ti)</td>
    </tr>
    <tr>
      <td>MBPO-Optimistic</td>
      <td>13.7 +/- 0.35 min (Time per 100k steps, 5 ensembles, GPU: NVIDIA GeForce RTX 2080 Ti)</td>
    </tr>
    <tr>
      <td>Dreamer</td>
      <td>42.24 +/- 0.95 min (Time per 100k steps, GPU: NVIDIA GeForce RTX 4090)</td>
    </tr>
    <tr>
      <td>Dreamer-Optimistic</td>
      <td>46.32 +/- 0.34 min (Time per 100k steps, 5 ensembles, GPU: NVIDIA GeForce RTX 4090)</td>
    </tr>
  </tbody>
</table>

<h3 id="sombrl-enables-sample-efficient-online-learning-on-hw">SOMBRL enables sample-efficient online learning on HW!</h3>
<p>We also apply SOMBRL on a real-world HW experiment. Here the task for the agent is to perform a drifting manneuver and park the highly dynamic RC car. The reward for this task is sparse. We compare SOMBRL to the SOTA model-based RL baseline: <a href="https://arxiv.org/abs/2403.16644" rel="external nofollow noopener" target="_blank">SimFSVGD</a>. Due to its naive exploration <a href="https://arxiv.org/abs/2403.16644" rel="external nofollow noopener" target="_blank">SimFSVGD</a> gets stuck at a local optima whereas SOMBRL finds the optimal solution.</p>
<div style="text-align: center;">
<img src="/assets/slides/sombrl/rccar_montage.gif" width="720">
</div>

<h2 id="bibtex">Bibtex</h2>

<div class="code-container" style="position: relative;">
    <button id="copy-btn" onclick="copyToClipboard()" style="position: absolute; top: 10px; right: 10px; background-color: #f0f0f0; border: none; padding: 5px 10px; cursor: pointer; border-radius: 5px;">
        <i id="copy-icon" class="fa fa-copy" style="font-size: 16px;"></i>
    </button>
    <pre id="bibtex-entry"><code>
@article{sukhija2025somrbl,
  title={SOMBRL: Scalable and Optimistic Model-Based RL},
  author={Sukhija, Bhavya and Treven, Lenart and Sferrazza, Carmelo and Dörfler, Florian and Abbeel, Pieter and Krause, Andreas},
  journal={NeurIPS},
  year={2025}
}
    </code></pre>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"></script>

<script>
function copyToClipboard() {
    var codeText = document.getElementById('bibtex-entry').innerText;
    var textArea = document.createElement('textarea');
    textArea.value = codeText;
    document.body.appendChild(textArea);
    textArea.select();
    document.execCommand('copy');
    document.body.removeChild(textArea);

    // Change the icon to the checkmark
    var copyIcon = document.getElementById('copy-icon');
    copyIcon.classList.remove('fa-copy');
    copyIcon.classList.add('fa-check');  // Switch to checkmark icon

    // Change it back to the copy icon after 2 seconds
    setTimeout(function() {
        copyIcon.classList.remove('fa-check');
        copyIcon.classList.add('fa-copy');  // Switch back to copy icon
    }, 350);
}
</script>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Bhavya  Sukhija. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
