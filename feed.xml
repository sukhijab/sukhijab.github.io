<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sukhijab.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sukhijab.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-12-16T23:07:43+00:00</updated><id>https://sukhijab.github.io/feed.xml</id><title type="html">Bhavya Sukhija</title><subtitle>Personal webpage of Bhavya Sukhija.
</subtitle><entry><title type="html">Tuning Legged Locomotion Controllers via Safe Bayesian Optimization</title><link href="https://sukhijab.github.io/research-post/2023/07/01/safeBO.html" rel="alternate" type="text/html" title="Tuning Legged Locomotion Controllers via Safe Bayesian Optimization" /><published>2023-07-01T10:25:00+00:00</published><updated>2023-07-01T10:25:00+00:00</updated><id>https://sukhijab.github.io/research-post/2023/07/01/safeBO</id><content type="html" xml:base="https://sukhijab.github.io/research-post/2023/07/01/safeBO.html"><![CDATA[<p>In this paper, we present a data-driven strategy to simplify the deployment of
model-based controllers in legged robotic hardware platforms. Our approach
leverages a model-free safe learning algorithm to automate the tuning of control
gains, addressing the mismatch between the simplified model used in the control
formulation and the real system. This method substantially mitigates the risk of
hazardous interactions with the robot by sample-efficiently optimizing parameters
within a probably safe region. Additionally, we extend the applicability of our
approach to incorporate the different gait parameters as contexts, leading to a safe,
sample-efficient exploration algorithm capable of tuning a motion controller for
diverse gait patterns. We validate our method through simulation and hardware
experiments, where we demonstrate that the algorithm obtains superior performance
on tuning a model-based motion controller for multiple gaits safely.</p>

<p>The paper is available <a href="https://arxiv.org/pdf/2306.07092.pdf"> here, </a> and below is a video demonstration.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/pceHWpFr3ng" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>]]></content><author><name></name></author><category term="research-post" /><category term="reinforcement-learning" /><category term="robotics" /><category term="safe-learnings" /><summary type="html"><![CDATA[A quick summary of my work about tuning legged locomotion controllers via safe Bayesian optimization.]]></summary></entry><entry><title type="html">Optimistic Active Exploration of Dynamical Systems</title><link href="https://sukhijab.github.io/research-post/2023/06/21/opax.html" rel="alternate" type="text/html" title="Optimistic Active Exploration of Dynamical Systems" /><published>2023-06-21T10:25:00+00:00</published><updated>2023-06-21T10:25:00+00:00</updated><id>https://sukhijab.github.io/research-post/2023/06/21/opax</id><content type="html" xml:base="https://sukhijab.github.io/research-post/2023/06/21/opax.html"><![CDATA[<p>Reinforcement learning algorithms commonly seek to optimize policies for solving one particular task. How should we explore an unknown dynamical system such that the estimated model allows us to solve multiple downstream tasks in a zero-shot manner? In this paper, we address this challenge, by developing an algorithm – OPAX – for active exploration. OPAX uses well-calibrated probabilistic models to quantify the epistemic uncertainty about the unknown dynamics. It optimistically – w.r.t. to plausible dynamics – maximizes the information gain between the unknown dynamics and state observations. We show how the resulting optimization problem can be reduced to an optimal control problem that can be solved at each episode using standard approaches. We analyze our algorithm for general models, and, in the case of Gaussian process dynamics, we give a sample complexity bound and show that the epistemic uncertainty converges to zero. In our experiments, we compare OPAX with other heuristic active exploration approaches on several environments. Our experiments show that OPAX is not only theoretically sound but also performs well for zero-shot planning on novel downstream tasks.</p>

<p>The paper is available <a href="https://arxiv.org/pdf/2306.12371.pdf"> here </a> and below is a small video demonstration of our simulation results.</p>

<div class="row mt-3">
        

<figure>

  

  <video src="/assets/video/opax_videos.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" />

  

</figure>

</div>
<div class="caption">
    Video demonstration of OPAX.
</div>]]></content><author><name></name></author><category term="research-post" /><category term="reinforcement-learning" /><summary type="html"><![CDATA[A quick summary of my work about optimistic active exploration of dynamical systems.]]></summary></entry><entry><title type="html">Hallucinated Adversarial Control for Conservative Offline Policy Evaluation</title><link href="https://sukhijab.github.io/research-post/2023/02/01/hambo.html" rel="alternate" type="text/html" title="Hallucinated Adversarial Control for Conservative Offline Policy Evaluation" /><published>2023-02-01T10:25:00+00:00</published><updated>2023-02-01T10:25:00+00:00</updated><id>https://sukhijab.github.io/research-post/2023/02/01/hambo</id><content type="html" xml:base="https://sukhijab.github.io/research-post/2023/02/01/hambo.html"><![CDATA[<p>We study the problem of conservative off-policy evaluation (COPE) where given an offline dataset of environment interactions, collected by other agents, we seek to obtain a (tight) lower bound on a policy’s performance. This is crucial when deciding whether a given policy satisfies certain minimal performance/safety criteria before it can be deployed in the real world. To this end, we introduce HAMBO, which builds on an uncertainty-aware learned model of the transition dynamics. To form a conservative estimate of the policy’s performance, HAMBO hallucinates worst-case trajectories that the policy may take, within the margin of the models’ epistemic confidence regions. We prove that the resulting COPE estimates are valid lower bounds, and, under regularity conditions, show their convergence to the true expected return. Finally, we discuss scalable variants of our approach based on Bayesian Neural Networks and empirically demonstrate that they yield reliable and tight lower bounds in various continuous control environments.</p>

<p>The paper is available <a href="https://proceedings.mlr.press/v216/rothfuss23a/rothfuss23a.pdf"> here. </a></p>]]></content><author><name></name></author><category term="research-post" /><category term="reinforcement-learning" /><summary type="html"><![CDATA[A quick summary of my work about hallucinated adversarial control for conservative offline policy evaluation.]]></summary></entry><entry><title type="html">Gradient-Based Trajectory Optimization With Learned Dynamics</title><link href="https://sukhijab.github.io/research-post/2022/10/21/gradient-based-TO.html" rel="alternate" type="text/html" title="Gradient-Based Trajectory Optimization With Learned Dynamics" /><published>2022-10-21T10:25:00+00:00</published><updated>2022-10-21T10:25:00+00:00</updated><id>https://sukhijab.github.io/research-post/2022/10/21/gradient-based-TO</id><content type="html" xml:base="https://sukhijab.github.io/research-post/2022/10/21/gradient-based-TO.html"><![CDATA[<p>Trajectory optimization methods have achieved an exceptional level of performance on real-world robots in recent years. These methods heavily rely on accurate analytical models of the dynamics, yet some aspects of the physical world can only be captured to a limited extent. An alternative approach is to leverage machine learning techniques to learn a differentiable dynamics model of the system from data. In this work, we use trajectory optimization and model learning for performing highly dynamic and complex tasks with robotic systems in absence of accurate analytical models of the dynamics. We show that a neural network can model highly nonlinear behaviors accurately for large time horizons, from data collected in only 25 minutes of interactions on two distinct robots: (i) the Boston Dynamics Spot and an (ii) RC car. Furthermore, we use the gradients of the neural network to perform gradient-based trajectory optimization. In our hardware experiments, we demonstrate that our learned model can represent complex dynamics for both the Spot and Radio-controlled (RC) car, and gives good performance in combination with trajectory optimization methods.</p>

<p>See video  <a href="http://crl.ethz.ch/videos/spot_icra_compressed_final.mp4"> here.</a> The paper is available <a href="https://arxiv.org/pdf/2204.04558.pdf"> here.</a></p>]]></content><author><name></name></author><category term="research-post" /><category term="robotics" /><summary type="html"><![CDATA[A quick summary of my work about Gradient-Based Trajectory Optimization With Learned Dynamics.]]></summary></entry><entry><title type="html">Safe Exploration for Dynamical Systems</title><link href="https://sukhijab.github.io/research-post/2022/05/08/gosafeopt.html" rel="alternate" type="text/html" title="Safe Exploration for Dynamical Systems" /><published>2022-05-08T10:25:00+00:00</published><updated>2022-05-08T10:25:00+00:00</updated><id>https://sukhijab.github.io/research-post/2022/05/08/gosafeopt</id><content type="html" xml:base="https://sukhijab.github.io/research-post/2022/05/08/gosafeopt.html"><![CDATA[<p>Learning optimal control policies directly on physical systems is challenging.
Even a single failure can lead to costly hardware damage. Most existing
model-free learning methods that guarantee safety, i.e., no failures, during
exploration are limited to local optima. This work proposes GoSafeOpt as the
first provably safe and optimal algorithm that can safely discover globally
optimal policies for systems with high-dimensional state space. We demonstrate
the superiority of GoSafeOpt over competing model-free safe learning methods in
simulation and hardware experiments on a robot arm.</p>

<p>Access hardware results <a href="/GoSafeOpt/main_project.html"> here</a> and software results <a href="/GoSafeOpt/software_experiments.html"> here</a>. The paper is available <a href="https://www.sciencedirect.com/science/article/pii/S0004370223000681"> here</a>.</p>]]></content><author><name></name></author><category term="research-post" /><category term="safe-learning" /><category term="robotics" /><category term="reinforcement-learning" /><summary type="html"><![CDATA[A quick summary of my work about globally safe exploration.]]></summary></entry></feed>